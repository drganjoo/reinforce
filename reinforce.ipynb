{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "rl",
   "display_name": "Python 3.8.8 64-bit ('rl': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### 1. Import the Necessary Packages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "source": [
    "## Define Policy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size=4, h_size=16, a_size=2):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim = 1 if len(x.shape) > 1 else 0)\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().to(self.fc1.weight.device)\n",
    "        probs = self(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "source": [
    "## Run Episodes (One episode a time not batch)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Episode 100\tAverage Score: 29.59\n",
      "Episode 200\tAverage Score: 48.25\n",
      "Episode 300\tAverage Score: 52.83\n",
      "Episode 400\tAverage Score: 59.80\n",
      "Episode 500\tAverage Score: 59.14\n",
      "Episode 600\tAverage Score: 58.05\n",
      "Episode 700\tAverage Score: 81.31\n",
      "Episode 800\tAverage Score: 62.43\n",
      "Episode 900\tAverage Score: 57.80\n",
      "Episode 1000\tAverage Score: 53.40\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def run_episode(policy, env, max_t):\n",
    "    saved_log_probs = []\n",
    "    rewards = []\n",
    "    state = env.reset()\n",
    "    for t in range(max_t):\n",
    "        action, log_prob = policy.act(state)\n",
    "        saved_log_probs.append(log_prob)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        if done:\n",
    "            break \n",
    "    return rewards, saved_log_probs\n",
    "\n",
    "def reinforce(policy, env, n_episodes=1000, max_t=1000, gamma=1.0, print_every=100):\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "    \n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        rewards, saved_log_probs = run_episode(policy, env, max_t)\n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "        \n",
    "        discounts = gamma ** np.arange(len(rewards))\n",
    "        R = np.sum(discounts * rewards)\n",
    "        \n",
    "        log_rewards = [-log_prob * R for log_prob in saved_log_probs]\n",
    "        policy_loss = torch.stack(log_rewards, dim = 0).sum(dim = 0)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        if np.mean(scores_deque)>=195.0:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_deque)))\n",
    "            break\n",
    "        \n",
    "    return scores\n",
    "    \n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(0)\n",
    "policy = Policy().to(device)\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(0)\n",
    "\n",
    "scores = reinforce(policy, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}